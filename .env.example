# ═══════════════════════════════════════════════════════════════════════════
# DocBrain Configuration
# Copy this to .env and fill in your values: cp .env.example .env
#
# How configuration works:
#   YAML defaults are baked into the Docker image (config/default.yaml +
#   config/production.yaml). You only need .env for secrets and values
#   specific to your deployment. Environment variables always win over YAML.
# ═══════════════════════════════════════════════════════════════════════════

# ── Infrastructure ────────────────────────────────────────────────────────
POSTGRES_USER=docbrain
POSTGRES_PASSWORD=change-me-in-production
POSTGRES_DB=docbrain
# DATABASE_URL is set automatically by docker-compose.yml — override only if using an external DB
# DATABASE_URL=postgresql://docbrain:change-me-in-production@postgres:5432/docbrain

OPENSEARCH_URL=http://opensearch:9200
REDIS_URL=redis://redis:6379

# ── LLM Provider (choose one: bedrock | anthropic | openai | ollama) ─────
#
# OPTION A: AWS Bedrock (recommended for teams on AWS — uses existing credentials)
#   Requires: AWS credentials (env vars, ~/.aws/credentials, or instance profile)
#   Enable models in: https://console.aws.amazon.com/bedrock/home#/modelaccess
#
LLM_PROVIDER=bedrock
LLM_MODEL_ID=us.anthropic.claude-sonnet-4-5-20250929-v1:0
EMBED_PROVIDER=bedrock
EMBED_MODEL_ID=cohere.embed-v4:0
AWS_REGION=us-east-1
# AWS_ACCESS_KEY_ID=              # Optional — uses default credential chain if unset
# AWS_SECRET_ACCESS_KEY=
#
# OPTION B: Anthropic + OpenAI (best quality)
#   Requires: ANTHROPIC_API_KEY + OPENAI_API_KEY
#
# LLM_PROVIDER=anthropic
# ANTHROPIC_API_KEY=sk-ant-...
# LLM_MODEL_ID=claude-sonnet-4-5-20250929
# EMBED_PROVIDER=openai
# OPENAI_API_KEY=sk-...
# EMBED_MODEL_ID=text-embedding-3-small
#
# OPTION C: OpenAI only (single API key)
#   Requires: OPENAI_API_KEY
#
# LLM_PROVIDER=openai
# OPENAI_API_KEY=sk-...
# LLM_MODEL_ID=gpt-4o
# EMBED_PROVIDER=openai
# EMBED_MODEL_ID=text-embedding-3-small
#
# OPTION D: 100% Local with Ollama (no API keys, no data leaves your machine)
#   Requires: Ollama running locally (ollama pull llama3.1 && ollama pull nomic-embed-text)
#
# LLM_PROVIDER=ollama
# OLLAMA_BASE_URL=http://host.docker.internal:11434
# LLM_MODEL_ID=llama3.1
# EMBED_PROVIDER=ollama
# EMBED_MODEL_ID=nomic-embed-text
# OLLAMA_TLS_VERIFY=false          # Set to true to enforce TLS certificate validation
# OLLAMA_VISION_ENABLED=true       # Set to false if your Ollama model does not support vision

# ── Document Source (choose one: local | confluence | github) ────────────
SOURCE_TYPE=local
LOCAL_DOCS_PATH=/data/docs

# Confluence (if SOURCE_TYPE=confluence)
# CONFLUENCE_BASE_URL=https://your-domain.atlassian.net/wiki
# CONFLUENCE_USER_EMAIL=
# CONFLUENCE_API_TOKEN=
# CONFLUENCE_SPACE_KEYS=DOCS,ENG
# CONFLUENCE_API_VERSION=v2          # v2 for Cloud (default), v1 for self-hosted Data Center 7.x+
# CONFLUENCE_PAGE_LIMIT=0            # Max pages to ingest per space (0 = unlimited, default)
# CONFLUENCE_TLS_VERIFY=true         # Set to false if using self-signed or internal CA certs

# GitHub (if SOURCE_TYPE=github)
# GITHUB_REPO_URL=https://github.com/your-org/your-docs
# GITHUB_TOKEN=
# GITHUB_BRANCH=main

# ── Optional Features ────────────────────────────────────────────────────
# LLM_THINKING_BUDGET=10000          # Extended thinking token budget (0 = disabled)
# HAIKU_MODEL_ID=                    # Override the fast/cheap model used for image descriptions

# ── Image Extraction ─────────────────────────────────────────────────────
# Extracts and describes images from Confluence pages using vision LLM.
# IMAGE_EXTRACTION_ENABLED=true      # Set to false to disable image extraction entirely
# IMAGE_MAX_PER_PAGE=20              # Max images to process per Confluence page
# IMAGE_MIN_SIZE_BYTES=5120          # Skip images smaller than this (bytes, default: 5KB)
# IMAGE_MAX_SIZE_BYTES=10485760      # Skip images larger than this (bytes, default: 10MB)
# IMAGE_DOWNLOAD_TIMEOUT=30          # HTTP download timeout in seconds
# IMAGE_LLM_TIMEOUT=120              # LLM vision call timeout in seconds

# ── Slack Integration ────────────────────────────────────────────────────
# SLACK_BOT_TOKEN=xoxb-...
# SLACK_SIGNING_SECRET=

# Channel to post critical gap alerts after each analysis run (e.g. #docs-alerts).
# Only fires when new critical-severity gaps are found. Requires SLACK_BOT_TOKEN.
# SLACK_GAP_NOTIFICATION_CHANNEL=#docs-alerts

# Stale doc notifications — sends DMs to doc owners (requires Slack)
# NOTIFICATION_INTERVAL_HOURS=24
# NOTIFICATION_SPACE_FILTER=PLATFORM,SRE  # optional: limit to specific spaces

# ── Documentation Autopilot (gap analysis + draft generation) ────────────
# AUTOPILOT_ENABLED=false
# AUTOPILOT_GAP_ANALYSIS_INTERVAL_HOURS=6   # How often the background scheduler runs
# AUTOPILOT_LOOKBACK_DAYS=30                # Days of query history to analyse for gaps
# AUTOPILOT_CLUSTER_THRESHOLD=0.82          # Cosine similarity for clustering (0.65=loose, 0.85=strict)
# AUTOPILOT_MIN_CLUSTER_SIZE=3              # Min episodes in a cluster to consider it a gap
# AUTOPILOT_MIN_UNIQUE_USERS=2              # Min distinct users that must hit the same gap
# AUTOPILOT_MIN_NEGATIVE_RATIO=0.15         # Min fraction of queries on a topic that must be negative
# AUTOPILOT_MAX_CLUSTERS=50                 # Max gap clusters to persist per analysis run
# AUTOPILOT_MAX_EPISODES=500                # Max negative episodes to load per run

# ── Freshness Scoring ─────────────────────────────────────────────────────
# FRESHNESS_SCHEDULER_INTERVAL_HOURS=24     # How often freshness scores are recalculated

# ── Memory Consolidation ──────────────────────────────────────────────────
# CONSOLIDATION_INTERVAL_HOURS=6            # How often the memory consolidation job runs

# ── Data Retention ────────────────────────────────────────────────────────
# EPISODE_RETENTION_DAYS=90                 # Episode (query history) retention. Set to 0 to disable.
# AUDIT_RETENTION_DAYS=365                  # Audit log retention. Set to 0 to disable.

# ── RAG Pipeline ──────────────────────────────────────────────────────────
# RAG_CACHE_TTL_HOURS=24                    # How long to cache semantically identical answers
# RAG_CACHE_THRESHOLD=0.95                  # Cosine similarity threshold for cache hits

# ── OpenSearch Index Names ────────────────────────────────────────────────
# Only needed if you run multiple DocBrain instances sharing the same OpenSearch cluster.
# OPENSEARCH_INDEX=docbrain-chunks
# OPENSEARCH_EPISODE_INDEX=docbrain-episodes

# ── Server ────────────────────────────────────────────────────────────────
# SERVER_PORT=3000
# LOG_LEVEL=info                     # trace | debug | info | warn | error
# MAX_QUERY_LENGTH=4000              # Max chars for question/description inputs

# ── Web UI / CORS ─────────────────────────────────────────────────────────
# Set to your web UI domain in production (e.g. https://docbrain.yourco.com)
# CORS_ALLOWED_ORIGINS=http://localhost:3001

# ── Auth / Sessions ───────────────────────────────────────────────────────
# LOGIN_SESSION_TTL_HOURS=720        # Session lifetime (default: 720h = 30 days, 0 = no expiry)

# ── Self-Ingest ────────────────────────────────────────────────────────────
# Auto-ingest DocBrain's own docs so it can answer configuration questions about itself.
# DOCBRAIN_SELF_INGEST=false
# DOCBRAIN_DOCS_PATH=./docs

# ── SSO / OIDC (Enterprise) ───────────────────────────────────────────────
# OIDC_ISSUER_URL=                   # e.g. https://accounts.google.com
# OIDC_CLIENT_ID=
# OIDC_CLIENT_SECRET=
# OIDC_REDIRECT_URI=http://localhost:3000/auth/oidc/callback
